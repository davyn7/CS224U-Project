{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHAkcmy4T3GB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8584c20e-b0d2-48bb-ceae-57e24389b439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cs224u'...\n",
            "remote: Enumerating objects: 2233, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 2233 (delta 64), reused 96 (delta 51), pack-reused 2092\u001b[K\n",
            "Receiving objects: 100% (2233/2233), 41.50 MiB | 21.78 MiB/s, done.\n",
            "Resolving deltas: 100% (1363/1363), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/stanfordnlp/dsp (from -r cs224u/requirements.txt (line 15))\n",
            "  Cloning https://github.com/stanfordnlp/dsp to /tmp/pip-req-build-e6z7k81r\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/stanfordnlp/dsp /tmp/pip-req-build-e6z7k81r\n",
            "  Resolved https://github.com/stanfordnlp/dsp to commit 298fac8937fdf7326d1b85546ecd41f099ed33e2\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from -r cs224u/requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from -r cs224u/requirements.txt (line 2)) (1.10.1)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from -r cs224u/requirements.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from -r cs224u/requirements.txt (line 4)) (1.2.2)\n",
            "Requirement already satisfied: nltk>=3.7 in /usr/local/lib/python3.10/dist-packages (from -r cs224u/requirements.txt (line 5)) (3.8.1)\n",
            "Requirement already satisfied: pytest>=7.1 in /usr/local/lib/python3.10/dist-packages (from -r cs224u/requirements.txt (line 6)) (7.2.2)\n",
            "Collecting jupyter>=1.0.0 (from -r cs224u/requirements.txt (line 7))\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.10/dist-packages (from -r cs224u/requirements.txt (line 8)) (1.5.3)\n",
            "Collecting torch==1.13.1 (from -r cs224u/requirements.txt (line 9))\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m860.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.14.1 (from -r cs224u/requirements.txt (line 10))\n",
            "  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.26.1 (from -r cs224u/requirements.txt (line 11))\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==2.10.1 (from -r cs224u/requirements.txt (line 12))\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy==3.5.1 (from -r cs224u/requirements.txt (line 13))\n",
            "  Downloading spacy-3.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cohere (from -r cs224u/requirements.txt (line 16))\n",
            "  Downloading cohere-4.9.0-py3-none-any.whl (38 kB)\n",
            "Collecting openai (from -r cs224u/requirements.txt (line 17))\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->-r cs224u/requirements.txt (line 9)) (4.5.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1->-r cs224u/requirements.txt (line 9))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1->-r cs224u/requirements.txt (line 9))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1->-r cs224u/requirements.txt (line 9))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1->-r cs224u/requirements.txt (line 9))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1->-r cs224u/requirements.txt (line 10)) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1->-r cs224u/requirements.txt (line 10)) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1->-r cs224u/requirements.txt (line 11)) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.26.1->-r cs224u/requirements.txt (line 11))\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1->-r cs224u/requirements.txt (line 11)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1->-r cs224u/requirements.txt (line 11)) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1->-r cs224u/requirements.txt (line 11)) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.26.1->-r cs224u/requirements.txt (line 11))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1->-r cs224u/requirements.txt (line 11)) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r cs224u/requirements.txt (line 12)) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r cs224u/requirements.txt (line 12)) (2023.4.0)\n",
            "Collecting aiohttp (from datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19 (from datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (3.3.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->-r cs224u/requirements.txt (line 9)) (0.40.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->-r cs224u/requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->-r cs224u/requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.7->-r cs224u/requirements.txt (line 5)) (8.1.3)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=7.1->-r cs224u/requirements.txt (line 6)) (23.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=7.1->-r cs224u/requirements.txt (line 6)) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=7.1->-r cs224u/requirements.txt (line 6)) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=7.1->-r cs224u/requirements.txt (line 6)) (1.1.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=7.1->-r cs224u/requirements.txt (line 6)) (2.0.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (6.4.8)\n",
            "Collecting qtconsole (from jupyter>=1.0.0->-r cs224u/requirements.txt (line 7))\n",
            "  Downloading qtconsole-5.4.3-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (7.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->-r cs224u/requirements.txt (line 8)) (2022.7.1)\n",
            "Collecting backoff (from dsp-ml==0.1.5->-r cs224u/requirements.txt (line 15))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting ujson (from dsp-ml==0.1.5->-r cs224u/requirements.txt (line 15))\n",
            "  Downloading ujson-5.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r cs224u/requirements.txt (line 12)) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1->-r cs224u/requirements.txt (line 10)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1->-r cs224u/requirements.txt (line 10)) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1->-r cs224u/requirements.txt (line 10)) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (0.0.4)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (6.3.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (3.6.4)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (3.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (2.1.2)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (2.14.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (4.9.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (5.3.0)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.2.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.7.4)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (5.8.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (1.2.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (21.3.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (1.5.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.16.0)\n",
            "Collecting qtpy>=2.0.1 (from qtconsole->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7))\n",
            "  Downloading QtPy-2.3.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jedi>=0.16 (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7))\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (4.8.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (3.3.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (2.16.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (4.3.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.2.6)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (2.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.8.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.19.3)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (2.21)\n",
            "Building wheels for collected packages: dsp-ml\n",
            "  Building wheel for dsp-ml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dsp-ml: filename=dsp_ml-0.1.5-py3-none-any.whl size=37803 sha256=6f34cfefcd4fadbedeb6c1b1ad0afbac9974e5edd07ca2ec93e95a124bf60ff4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-repvsm_y/wheels/b9/cc/21/df1431394d64701ffb9d4a5a20afe701a19275c3090b73fd0e\n",
            "Successfully built dsp-ml\n",
            "Installing collected packages: tokenizers, xxhash, ujson, qtpy, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, multidict, jedi, frozenlist, dill, backoff, async-timeout, yarl, responses, nvidia-cudnn-cu11, multiprocess, huggingface-hub, aiosignal, transformers, torch, aiohttp, torchvision, spacy, qtconsole, openai, cohere, datasets, jupyter, dsp-ml\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2+cu118\n",
            "    Uninstalling torchvision-0.15.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.2+cu118\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.2\n",
            "    Uninstalling spacy-3.5.2:\n",
            "      Successfully uninstalled spacy-3.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 backoff-2.2.1 cohere-4.9.0 datasets-2.10.1 dill-0.3.6 dsp-ml-0.1.5 frozenlist-1.3.3 huggingface-hub-0.15.1 jedi-0.18.2 jupyter-1.0.0 multidict-6.0.4 multiprocess-0.70.14 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 openai-0.27.8 qtconsole-5.4.3 qtpy-2.3.1 responses-0.18.0 spacy-3.5.1 tokenizers-0.13.3 torch-1.13.1 torchvision-0.14.1 transformers-4.26.1 ujson-5.7.0 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "try: \n",
        "    # This library is our indicator that the required installs\n",
        "    # need to be done.\n",
        "    import datasets\n",
        "    root_path = '.'\n",
        "except ModuleNotFoundError:\n",
        "    !git clone https://github.com/cgpotts/cs224u/\n",
        "    !pip install -r cs224u/requirements.txt\n",
        "    root_path = 'dsp'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDMjXCc5ARZl",
        "outputId": "15536471-4f30-435c-e211-2eea09d1d3c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a04cb488-cd40-4f9d-b884-8ff83b012042"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "from datasets import load_dataset\n",
        "import openai\n",
        "import os\n",
        "import dsp\n",
        "import pandas as pd\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9da704b-d27b-480a-93b5-e16cf7c51803"
      },
      "outputs": [],
      "source": [
        "os.environ[\"DSP_NOTEBOOK_CACHEDIR\"] = os.path.join(root_path, 'cache')\n",
        "\n",
        "openai_key = os.getenv('OPEN_API_KEY')  # or replace with your API key (optional)\n",
        "\n",
        "cohere_key = os.getenv('COHERE_API_KEY')  # or replace with your API key (optional)\n",
        "\n",
        "colbert_server = 'http://ec2-44-228-128-229.us-west-2.compute.amazonaws.com:8893/api/search'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = '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'"
      ],
      "metadata": {
        "id": "WxXnAS1TfnZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c118b014-e13f-433d-ad60-074636c7e738"
      },
      "outputs": [],
      "source": [
        "lm = dsp.GPT3(model='text-davinci-001', api_key=openai_key)\n",
        "\n",
        "# Options for Cohere: command-medium-nightly, command-xlarge-nightly\n",
        "#lm = dsp.Cohere(model='command-medium-nightly', api_key=cohere_key)\n",
        "\n",
        "# rm = dsp.ColBERTv2(url=colbert_server)\n",
        "\n",
        "dsp.settings.configure(lm=lm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_with_defs_df = pd.read_csv('/content/drive/MyDrive/CS224U/test_with_defs.csv')"
      ],
      "metadata": {
        "id": "ca28xdgtywZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_slang_tags_df = pd.read_csv('/content/drive/MyDrive/CS224U/new_combined_data.csv')"
      ],
      "metadata": {
        "id": "VrsfFRSwDw_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text(s):\n",
        "    lst = ast.literal_eval(s)\n",
        "    return \"\".join([\" \"+i if not i.startswith((\"'\", \",\", \".\", \":\", \";\", \"!\", \"?\")) else i for i in lst]).strip()"
      ],
      "metadata": {
        "id": "cM2j2fmyMgXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slang_phrase = predicted_slang_tags_df.iloc[9]['word']\n",
        "process_text(slang_phrase)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7C7Zm4fEFnt1",
        "outputId": "0170356a-839a-4072-f413-947b54ed5120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'101 scoop, the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Word = dsp.Type(\n",
        "    prefix=\"Slang word:\",\n",
        "    desc=\"${slang word to be replaced}\",\n",
        ")\n",
        "\n",
        "Definition = dsp.Type(prefix=\"Definition:\", desc=\"${the definition of the word}\")\n",
        "\n",
        "Sentence = dsp.Type(\n",
        "    prefix=\"Sentence:\",\n",
        "    desc=\"${a sentence containing the word}\"\n",
        ")\n",
        "\n",
        "Edited = dsp.Type(\n",
        "    prefix=\"Edited:\",\n",
        "    desc=\"${same sentence but with the word replaced with its definition given the context}\"\n",
        ")\n",
        "\n",
        "slang_translate_template = dsp.Template(\n",
        "    instructions='Given a word and its definition, replace that word in the given sentence with the surrounding context.',\n",
        "    word=Word(), definition=Definition(), sentence=Sentence(), edited=Edited()\n",
        ")"
      ],
      "metadata": {
        "id": "wI3WjpjPw5Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_with_defs_df.head(40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DED-xIDR33ZY",
        "outputId": "bbdab8c3-202a-4673-96a5-970923af3e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n",
              "0              0         22572       22572   \n",
              "1              1          9925        9925   \n",
              "2              2         17841       17841   \n",
              "3              3         41673       41673   \n",
              "4              4         15039       15039   \n",
              "5              5         45355       45355   \n",
              "6              6         31875       31875   \n",
              "7              7         13692       13692   \n",
              "8              8          4889        4889   \n",
              "9              9         20526       20526   \n",
              "10            10         31658       31658   \n",
              "11            11         32882       32882   \n",
              "12            12         18694       18694   \n",
              "13            13         34592       34592   \n",
              "14            14         19547       19547   \n",
              "15            15         40613       40613   \n",
              "16            16          6278        6278   \n",
              "17            17         25018       25018   \n",
              "18            18         44356       44356   \n",
              "19            19         18420       18420   \n",
              "20            20         13145       13145   \n",
              "21            21         24173       24173   \n",
              "22            22         35364       35364   \n",
              "23            23          7450        7450   \n",
              "24            24         27129       27129   \n",
              "25            25          1653        1653   \n",
              "26            26          4307        4307   \n",
              "27            27         38934       38934   \n",
              "28            28         26856       26856   \n",
              "29            29         40709       40709   \n",
              "30            30          9347        9347   \n",
              "31            31         11543       11543   \n",
              "32            32         37984       37984   \n",
              "33            33         27542       27542   \n",
              "34            34          5470        5470   \n",
              "35            35         43177       43177   \n",
              "36            36          3389        3389   \n",
              "37            37         49621       49621   \n",
              "38            38         40389       40389   \n",
              "39            39         11858       11858   \n",
              "\n",
              "                                                 text  sentiment  \\\n",
              "0    George Osborne writing in the Guardian  Not s...          2   \n",
              "1   Hear keynote speaker Doug Wead this Saturday u...          1   \n",
              "2     MammaTort I have no idea who that is  Was th...          2   \n",
              "3   i ll make you some more justin bieber edits to...          2   \n",
              "4   Feels like a Monday     FM Kirby Center for th...          2   \n",
              "5    Nowacking I though you may want that you and ...          1   \n",
              "6    I know there may be Jews that still hate Chri...          2   \n",
              "7   Knicks vs Celtics oct  20 at the times union  ...          2   \n",
              "8    Shaker is going to have school tomorrow  Ther...          1   \n",
              "9   Break out the Sunday red  Tiger woods is back ...          1   \n",
              "10    NiallOfficial Niall please follow me  my bir...          1   \n",
              "11        Nicki s drag at the VMA s blessed my Monday          1   \n",
              "12  Jan Hooks  Batman Returns  Bilingual   Import ...          2   \n",
              "13   Keris Their faces in the 2nd one are super cu...          1   \n",
              "14  They say you are what you eat u002c but it u20...          1   \n",
              "15    BBCNews BBC must be disappointed about   Blo...          0   \n",
              "16  tomorrow i u2019m scheduled to put up the Xmas...          0   \n",
              "17  John Kasich applied all the rote Republican do...          0   \n",
              "18  David Beckham may have just passed Taylor Swif...          1   \n",
              "19  Just informed my mother of the fact that I m s...          2   \n",
              "20  can u2019t wait until saturday morning when im...          0   \n",
              "21   Still hyped rn  Kris Bryant is my favorite pl...          1   \n",
              "22  Republican frontrunner Donald Trump said Thurs...          1   \n",
              "23  After 2 days  fuckuover is the 20th most playe...          2   \n",
              "24   A Rabbi Leads the Way in Uniting Jews  Christ...          2   \n",
              "25    Trending  Tim Tebow is now dating cave woman...          2   \n",
              "26   Still struggling with What s That Noise   If ...          0   \n",
              "27   or yuh kno  keep giving thanks 4 First Nation...          0   \n",
              "28  Sam Smith concert tomorrow got rescheduled and...          0   \n",
              "29   life is good  school is okay  im probably goi...          1   \n",
              "30   Monster HOND I hear that SANZAR have confirme...          0   \n",
              "31  We all went to see Rise of the Planet of the A...          1   \n",
              "32   Buenos Jueves   good thursday with David Bowi...          1   \n",
              "33   Today in Beatles History  March 15  1999   Pa...          1   \n",
              "34  Kevin Hart show at MSG on November 10th is goi...          1   \n",
              "35  Jon Moss ref for Saturday  That s every 50 50 ...          1   \n",
              "36  In three hours time our boys will be taking on...          1   \n",
              "37  Kootenay Pride Parade 2015 3pm SEPTEMBER 6  20...          2   \n",
              "38  watching labyrinth for the 1st time ever   Dav...          0   \n",
              "39  Just found all of ST  Voyager on Netflix  I kn...          1   \n",
              "\n",
              "                word                                        definitions  \n",
              "0         B584cQvunx                                                NaN  \n",
              "1                NaN                                                NaN  \n",
              "2          MammaTort                                                NaN  \n",
              "3      justin bieber                                                NaN  \n",
              "4          Monday FM                                                NaN  \n",
              "5          Nowacking                                                NaN  \n",
              "6                NaN                                                NaN  \n",
              "7            Bethere                                                NaN  \n",
              "8             Shaker                                                NaN  \n",
              "9    red Tiger woods                                                NaN  \n",
              "10     NiallOfficial                                                NaN  \n",
              "11              drag  a person's story: what they would have you bel...  \n",
              "12               Jan                                                NaN  \n",
              "13             Keris                                                NaN  \n",
              "14               NaN                                                NaN  \n",
              "15           BBCNews                                                NaN  \n",
              "16            u2019m                                                NaN  \n",
              "17  Republican dogma                                                NaN  \n",
              "18     David Beckham                                                NaN  \n",
              "19   Vampire Diaries                                                NaN  \n",
              "20            u2019t                                                NaN  \n",
              "21       Kris Bryant                                                NaN  \n",
              "22               NaN                                                NaN  \n",
              "23         fuckuover                                                NaN  \n",
              "24             Rabbi                                                NaN  \n",
              "25             Tebow                                                NaN  \n",
              "26               NaN                                                NaN  \n",
              "27             First                                                NaN  \n",
              "28               NaN                                                NaN  \n",
              "29               tfa                                                NaN  \n",
              "30            SANZAR                                                NaN  \n",
              "31      chimp Caesar                                                NaN  \n",
              "32           rV29uzF                                                NaN  \n",
              "33               NaN                                                NaN  \n",
              "34        Kevin Hart                                                NaN  \n",
              "35          Jon Moss                                                NaN  \n",
              "36               NaN                                                NaN  \n",
              "37      Pride Parade                                                NaN  \n",
              "38         labyrinth                                                NaN  \n",
              "39               NaN                                                NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9ffe4818-9546-422c-bbf0-682bb7104c97\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0.2</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>word</th>\n",
              "      <th>definitions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>22572</td>\n",
              "      <td>22572</td>\n",
              "      <td>George Osborne writing in the Guardian  Not s...</td>\n",
              "      <td>2</td>\n",
              "      <td>B584cQvunx</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>9925</td>\n",
              "      <td>9925</td>\n",
              "      <td>Hear keynote speaker Doug Wead this Saturday u...</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>17841</td>\n",
              "      <td>17841</td>\n",
              "      <td>MammaTort I have no idea who that is  Was th...</td>\n",
              "      <td>2</td>\n",
              "      <td>MammaTort</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>41673</td>\n",
              "      <td>41673</td>\n",
              "      <td>i ll make you some more justin bieber edits to...</td>\n",
              "      <td>2</td>\n",
              "      <td>justin bieber</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>15039</td>\n",
              "      <td>15039</td>\n",
              "      <td>Feels like a Monday     FM Kirby Center for th...</td>\n",
              "      <td>2</td>\n",
              "      <td>Monday FM</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>45355</td>\n",
              "      <td>45355</td>\n",
              "      <td>Nowacking I though you may want that you and ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Nowacking</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>31875</td>\n",
              "      <td>31875</td>\n",
              "      <td>I know there may be Jews that still hate Chri...</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>13692</td>\n",
              "      <td>13692</td>\n",
              "      <td>Knicks vs Celtics oct  20 at the times union  ...</td>\n",
              "      <td>2</td>\n",
              "      <td>Bethere</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>4889</td>\n",
              "      <td>4889</td>\n",
              "      <td>Shaker is going to have school tomorrow  Ther...</td>\n",
              "      <td>1</td>\n",
              "      <td>Shaker</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>20526</td>\n",
              "      <td>20526</td>\n",
              "      <td>Break out the Sunday red  Tiger woods is back ...</td>\n",
              "      <td>1</td>\n",
              "      <td>red Tiger woods</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>31658</td>\n",
              "      <td>31658</td>\n",
              "      <td>NiallOfficial Niall please follow me  my bir...</td>\n",
              "      <td>1</td>\n",
              "      <td>NiallOfficial</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>32882</td>\n",
              "      <td>32882</td>\n",
              "      <td>Nicki s drag at the VMA s blessed my Monday</td>\n",
              "      <td>1</td>\n",
              "      <td>drag</td>\n",
              "      <td>a person's story: what they would have you bel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>18694</td>\n",
              "      <td>18694</td>\n",
              "      <td>Jan Hooks  Batman Returns  Bilingual   Import ...</td>\n",
              "      <td>2</td>\n",
              "      <td>Jan</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>34592</td>\n",
              "      <td>34592</td>\n",
              "      <td>Keris Their faces in the 2nd one are super cu...</td>\n",
              "      <td>1</td>\n",
              "      <td>Keris</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>19547</td>\n",
              "      <td>19547</td>\n",
              "      <td>They say you are what you eat u002c but it u20...</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>40613</td>\n",
              "      <td>40613</td>\n",
              "      <td>BBCNews BBC must be disappointed about   Blo...</td>\n",
              "      <td>0</td>\n",
              "      <td>BBCNews</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>6278</td>\n",
              "      <td>6278</td>\n",
              "      <td>tomorrow i u2019m scheduled to put up the Xmas...</td>\n",
              "      <td>0</td>\n",
              "      <td>u2019m</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>25018</td>\n",
              "      <td>25018</td>\n",
              "      <td>John Kasich applied all the rote Republican do...</td>\n",
              "      <td>0</td>\n",
              "      <td>Republican dogma</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>44356</td>\n",
              "      <td>44356</td>\n",
              "      <td>David Beckham may have just passed Taylor Swif...</td>\n",
              "      <td>1</td>\n",
              "      <td>David Beckham</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>18420</td>\n",
              "      <td>18420</td>\n",
              "      <td>Just informed my mother of the fact that I m s...</td>\n",
              "      <td>2</td>\n",
              "      <td>Vampire Diaries</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>13145</td>\n",
              "      <td>13145</td>\n",
              "      <td>can u2019t wait until saturday morning when im...</td>\n",
              "      <td>0</td>\n",
              "      <td>u2019t</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>24173</td>\n",
              "      <td>24173</td>\n",
              "      <td>Still hyped rn  Kris Bryant is my favorite pl...</td>\n",
              "      <td>1</td>\n",
              "      <td>Kris Bryant</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>35364</td>\n",
              "      <td>35364</td>\n",
              "      <td>Republican frontrunner Donald Trump said Thurs...</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>7450</td>\n",
              "      <td>7450</td>\n",
              "      <td>After 2 days  fuckuover is the 20th most playe...</td>\n",
              "      <td>2</td>\n",
              "      <td>fuckuover</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>27129</td>\n",
              "      <td>27129</td>\n",
              "      <td>A Rabbi Leads the Way in Uniting Jews  Christ...</td>\n",
              "      <td>2</td>\n",
              "      <td>Rabbi</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>1653</td>\n",
              "      <td>1653</td>\n",
              "      <td>Trending  Tim Tebow is now dating cave woman...</td>\n",
              "      <td>2</td>\n",
              "      <td>Tebow</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>4307</td>\n",
              "      <td>4307</td>\n",
              "      <td>Still struggling with What s That Noise   If ...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>38934</td>\n",
              "      <td>38934</td>\n",
              "      <td>or yuh kno  keep giving thanks 4 First Nation...</td>\n",
              "      <td>0</td>\n",
              "      <td>First</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>28</td>\n",
              "      <td>26856</td>\n",
              "      <td>26856</td>\n",
              "      <td>Sam Smith concert tomorrow got rescheduled and...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>29</td>\n",
              "      <td>40709</td>\n",
              "      <td>40709</td>\n",
              "      <td>life is good  school is okay  im probably goi...</td>\n",
              "      <td>1</td>\n",
              "      <td>tfa</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>30</td>\n",
              "      <td>9347</td>\n",
              "      <td>9347</td>\n",
              "      <td>Monster HOND I hear that SANZAR have confirme...</td>\n",
              "      <td>0</td>\n",
              "      <td>SANZAR</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>31</td>\n",
              "      <td>11543</td>\n",
              "      <td>11543</td>\n",
              "      <td>We all went to see Rise of the Planet of the A...</td>\n",
              "      <td>1</td>\n",
              "      <td>chimp Caesar</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>32</td>\n",
              "      <td>37984</td>\n",
              "      <td>37984</td>\n",
              "      <td>Buenos Jueves   good thursday with David Bowi...</td>\n",
              "      <td>1</td>\n",
              "      <td>rV29uzF</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>33</td>\n",
              "      <td>27542</td>\n",
              "      <td>27542</td>\n",
              "      <td>Today in Beatles History  March 15  1999   Pa...</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>34</td>\n",
              "      <td>5470</td>\n",
              "      <td>5470</td>\n",
              "      <td>Kevin Hart show at MSG on November 10th is goi...</td>\n",
              "      <td>1</td>\n",
              "      <td>Kevin Hart</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>35</td>\n",
              "      <td>43177</td>\n",
              "      <td>43177</td>\n",
              "      <td>Jon Moss ref for Saturday  That s every 50 50 ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Jon Moss</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>36</td>\n",
              "      <td>3389</td>\n",
              "      <td>3389</td>\n",
              "      <td>In three hours time our boys will be taking on...</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>37</td>\n",
              "      <td>49621</td>\n",
              "      <td>49621</td>\n",
              "      <td>Kootenay Pride Parade 2015 3pm SEPTEMBER 6  20...</td>\n",
              "      <td>2</td>\n",
              "      <td>Pride Parade</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>38</td>\n",
              "      <td>40389</td>\n",
              "      <td>40389</td>\n",
              "      <td>watching labyrinth for the 1st time ever   Dav...</td>\n",
              "      <td>0</td>\n",
              "      <td>labyrinth</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>39</td>\n",
              "      <td>11858</td>\n",
              "      <td>11858</td>\n",
              "      <td>Just found all of ST  Voyager on Netflix  I kn...</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ffe4818-9546-422c-bbf0-682bb7104c97')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9ffe4818-9546-422c-bbf0-682bb7104c97 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9ffe4818-9546-422c-bbf0-682bb7104c97');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# final_test_df = test_with_defs_df.loc[test_with_defs_df['definitions'] != \"\" & (test_with_defs_df['definitions'].notnull())]\n",
        "defs_test_df = test_with_defs_df[~test_with_defs_df['definitions'].isna()]\n",
        "print(len(defs_test_df))\n",
        "\n",
        "no_defs_test_df = test_with_defs_df[test_with_defs_df['definitions'].isna()].sample(450)\n",
        "print(len(no_defs_test_df))\n",
        "\n",
        "\n",
        "final_test_df = pd.concat([defs_test_df, no_defs_test_df], ignore_index=True)\n",
        "print(len(final_test_df))\n",
        "final_test_df.to_csv(\"/content/drive/MyDrive/CS224U/final-test-df.csv\", sep=\",\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IziF1TRx1RPQ",
        "outputId": "44b8de5e-11b5-491e-9cc1-e149033cfc50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n",
            "450\n",
            "500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset_df = predicted_slang_tags_df.sample(n=1000)\n",
        "subset_df.to_csv(\"/content/drive/MyDrive/CS224U/cs224u-sentiment-df.csv\", sep=\",\")"
      ],
      "metadata": {
        "id": "4FbAWYmMqdph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def generate_combinations(words):\n",
        "    combinations_list = []\n",
        "    for i in range(len(words)):\n",
        "        for j in range(i + 1, len(words) + 1):\n",
        "            combinations_list.append('-'.join(words[i:j]))\n",
        "    return combinations_list\n",
        "\n",
        "\n",
        "current_subset_df = pd.read_csv('/content/drive/MyDrive/CS224U/final-test-df.csv')\n",
        "dsp_slang_train = []\n",
        "for index, row in current_subset_df.iterrows():\n",
        "  # hyphenated_combinations = generate_combinations(row['word'])\n",
        "  # joined_slang_phrase = process_text(row['word'])\n",
        "  word = row['word']\n",
        "  if word == \"\":\n",
        "    word = None\n",
        "  \n",
        "  definition = row['definitions']\n",
        "  if definition == \"\":\n",
        "    definition = None\n",
        "\n",
        "  ex = dsp.Example(word=word, definition=definition, sentence=row['text'])\n",
        "  dsp_slang_train.append(ex)\n",
        "  \n",
        "print(len(dsp_slang_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0ts7W6P5woo",
        "outputId": "0f5371c5-a668-40a9-f8e9-3748fb377cad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_combinations(words):\n",
        "    combinations_list = []\n",
        "    for i in range(len(words)):\n",
        "        for j in range(i + 1, len(words) + 1):\n",
        "            combinations_list.append('-'.join(words[i:j]))\n",
        "    return combinations_list\n",
        "\n",
        "\n",
        "current_subset_df = pd.read_csv('/content/drive/MyDrive/CS224U/cs224u-subset-df.csv')\n",
        "dsp_slang_train = []\n",
        "for index, row in current_subset_df.iterrows():\n",
        "  hyphenated_combinations = generate_combinations(row['word'])\n",
        "  joined_slang_phrase = process_text(row['word'])\n",
        "  if joined_slang_phrase in row['usage']: #  or any(combo in row['usage'] for combo in hyphenated_combinations):\n",
        "    ex = dsp.Example(word=joined_slang_phrase, definition=row['definition'], sentence=row['usage'])\n",
        "    dsp_slang_train.append(ex)\n",
        "  else:\n",
        "    current_subset_df.drop(index, inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "Rpw0i34bvkNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(current_subset_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty5PHUPpDCYX",
        "outputId": "7097d3e1-1331-47c4-c5d9-e2d539b4f7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dsp_slang_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqgtPhz_DFlx",
        "outputId": "32b74ccf-4701-4d15-d928-16ac6f301c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dsp_slang_train[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tEQeo8C8Z1C",
        "outputId": "25cae67b-f01c-46c1-89c7-2ca1e36144d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'word': 'drag', 'definition': \"a person's story: what they would have you believe\", 'sentence': 'Nicki s drag at the VMA s blessed my Monday'}, {'word': 'dam', 'definition': '1.device used to block water ;; 2.mothers against dyslexia ;; 3.common misspelling of damn', 'sentence': 'Aaaaahhhhh dam  I am out of my favorite Tea  Damit I have to wait until tomorrow to buy it now   '}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset_df.head(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "ESNCWT2kTNV9",
        "outputId": "29bc1407-6e4f-4aa9-f626-f627c778db48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     word                                         definition  \\\n",
              "42864     ['badunkabunk']  it's more than bunk...it's badunkabunk. see ba...   \n",
              "85900   ['sheri', 'moon']  a really hot lass, appeared in ;; 1)The house ...   \n",
              "28272  ['fart', 'huffer']  A person who is interested in smelling farts i...   \n",
              "6598           ['goodie']                     \"hello\" or \"how are you doing\"   \n",
              "82042         ['gladius']  A short sword used by the Romans. They liked t...   \n",
              "23607          ['dvdato']  a sexual act involving an innocent lady and se...   \n",
              "3823           ['dialed']  extreme approval, great, awesome, fine or mean...   \n",
              "86702             ['emu']  Emus...The coolest bird in the world ;; THe an...   \n",
              "88859            ['wwaw']  Word Within a Word (WWAW) ;; A from of torture...   \n",
              "32386   ['blind', 'date']  An event where someone sets up 2 people who do...   \n",
              "\n",
              "                                                   usage  \\\n",
              "42864  awww man, i failed that midterm today... that'...   \n",
              "85900                           sheri moon is a hot lass   \n",
              "28272  i was amazed to see jimmy huffing his dog's si...   \n",
              "6598                           goodie long time no see .   \n",
              "82042  there are four known types of gladius's, hispa...   \n",
              "23607               oh man i need some dvdato right now!   \n",
              "3823                               that girl is dialed .   \n",
              "86702  i saw an emu at the zoo it was planning it's a...   \n",
              "88859  wwaw is time consuming, utterley useless, and ...   \n",
              "32386  blind dates are sometimes good, usually bad, a...   \n",
              "\n",
              "                                                    tags  \n",
              "42864  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
              "85900                                      B I O O O O O  \n",
              "28272  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
              "6598                                         B O O O O O  \n",
              "82042  O O O O O O B O O O O O O O O O O O O O O O O ...  \n",
              "23607                                  O O O O O B O O O  \n",
              "3823                                           O O O B O  \n",
              "86702                      O O O B O O O O O O O O O O O  \n",
              "88859                          B O O O O O O O O O O O O  \n",
              "32386                    B O O O O O O O O O O O O O O O  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-15716f3c-325c-476e-b7b6-37b220018f68\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>definition</th>\n",
              "      <th>usage</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42864</th>\n",
              "      <td>['badunkabunk']</td>\n",
              "      <td>it's more than bunk...it's badunkabunk. see ba...</td>\n",
              "      <td>awww man, i failed that midterm today... that'...</td>\n",
              "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85900</th>\n",
              "      <td>['sheri', 'moon']</td>\n",
              "      <td>a really hot lass, appeared in ;; 1)The house ...</td>\n",
              "      <td>sheri moon is a hot lass</td>\n",
              "      <td>B I O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28272</th>\n",
              "      <td>['fart', 'huffer']</td>\n",
              "      <td>A person who is interested in smelling farts i...</td>\n",
              "      <td>i was amazed to see jimmy huffing his dog's si...</td>\n",
              "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6598</th>\n",
              "      <td>['goodie']</td>\n",
              "      <td>\"hello\" or \"how are you doing\"</td>\n",
              "      <td>goodie long time no see .</td>\n",
              "      <td>B O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82042</th>\n",
              "      <td>['gladius']</td>\n",
              "      <td>A short sword used by the Romans. They liked t...</td>\n",
              "      <td>there are four known types of gladius's, hispa...</td>\n",
              "      <td>O O O O O O B O O O O O O O O O O O O O O O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23607</th>\n",
              "      <td>['dvdato']</td>\n",
              "      <td>a sexual act involving an innocent lady and se...</td>\n",
              "      <td>oh man i need some dvdato right now!</td>\n",
              "      <td>O O O O O B O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3823</th>\n",
              "      <td>['dialed']</td>\n",
              "      <td>extreme approval, great, awesome, fine or mean...</td>\n",
              "      <td>that girl is dialed .</td>\n",
              "      <td>O O O B O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86702</th>\n",
              "      <td>['emu']</td>\n",
              "      <td>Emus...The coolest bird in the world ;; THe an...</td>\n",
              "      <td>i saw an emu at the zoo it was planning it's a...</td>\n",
              "      <td>O O O B O O O O O O O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88859</th>\n",
              "      <td>['wwaw']</td>\n",
              "      <td>Word Within a Word (WWAW) ;; A from of torture...</td>\n",
              "      <td>wwaw is time consuming, utterley useless, and ...</td>\n",
              "      <td>B O O O O O O O O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32386</th>\n",
              "      <td>['blind', 'date']</td>\n",
              "      <td>An event where someone sets up 2 people who do...</td>\n",
              "      <td>blind dates are sometimes good, usually bad, a...</td>\n",
              "      <td>B O O O O O O O O O O O O O O O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15716f3c-325c-476e-b7b6-37b220018f68')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-15716f3c-325c-476e-b7b6-37b220018f68 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-15716f3c-325c-476e-b7b6-37b220018f68');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dsp_slang_train[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3utUCmPD8Sb_",
        "outputId": "8f0588de-3283-45f3-b788-b0b34c09188c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'word': 'turd monkey', 'definition': '1) A phrase which is used to express disgust towards a person. Not to be mistaken with [monkey turd]. A turd monkey is generally someone who annoys or disgusts you, but can be used on anyone who generally pisses you off. ;; 2) A statue of a monkey made out of a [turd].', 'sentence': \"1 ) `` you complete and utter turd monkey.'' 2 ) `` that's a good turd monkey there, stan.''\"}, {'word': 'grumper', 'definition': 'A defecation that requires mass force to expel.', 'sentence': 'damn it, i just stepped in the biggest pile of grumper!'}, {'word': 'bagsy', 'definition': \"To stake a claim on an object or task. ;; Also 'bagsy not' - the opposite.\", 'sentence': 'bagsy mine. bagsy not in goal!'}, {'word': \"s'aight\", 'definition': \"Combination of it's alright in eboniks\", 'sentence': \"`` sorry i wrecked your car last night.'' `` s'aight''\"}, {'word': 'famu', 'definition': 'Bomb ass historically black college; A.K.A. Florida A & M University', 'sentence': \"famu's band is the best\"}, {'word': 'bail out', 'definition': 'to leave', 'sentence': \"i 'm gon na have to bail out early .\"}, {'word': 'homothug', 'definition': 'A person who acts gangsta but takes it in the ass see [faggot] ;; A male who wears pink and thinks its the shit it aint nigga ya damn homothug ;; see [dipset]', 'sentence': \"`` yo you know montay, that nigga turned into a homothug from going to prision''\"}, {'word': 'heya', 'definition': \"An incredibly informal greeting. Actually an abbreviation of 'Hey You!' ;; Shortened to convenience those of us too damned lazy to put definate spaces between our words...\", 'sentence': 'which is cuter, heya or brittana?'}, {'word': 'pre-nup', 'definition': \"Shorthand for 'Pre-nuptial agreement'. ;; -This is a contract signed by both partners of the marriage before it happens, limiting the rights of one partner or both on event of divorce. ;; Pre-nups are a long way from total acceptance anywhere in the world, but in the next ten years it is quite likely that more of these will find their ways into a mrriage.\", 'sentence': \"( argument between a husband and wife ) fuck you bitch, since your dumbass signed this pre-nup, that means that i keep the house and i do n't have to put up with your shit. so you and your kids can both get the fuck out! i'm the goddamn king of my domain!!!\"}, {'word': 'overmind', 'definition': \"The ruler of the Zerg race in the [Blizzard]'s game [StarCraft].\", 'sentence': 'in starcraft world one of the races is zerg. all creatures from that race were controled by an entity called overmind.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dsp_slang_train[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Boem7uTrSD2d",
        "outputId": "e525eebc-fdb1-4941-8afb-d0d66f83c80c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'word': \"['1']\", 'definition': '\"goodbye\"', 'sentence': \"i 'll talk to you later . 1 .\"}, {'word': \"['1']\", 'definition': '\"goodbye\"', 'sentence': 'see you later , 1 .'}, {'word': \"['1']\", 'definition': '\"goodbye\"', 'sentence': 'this 1 is done for .'}, {'word': \"['1']\", 'definition': '\"goodbye\"', 'sentence': 'we are going to get this 1 .'}, {'word': \"['1']\", 'definition': '\"goodbye\"', 'sentence': 'watching this 1 .'}, {'word': \"['1']\", 'definition': '\"goodbye\"', 'sentence': 'you are the 1 ( one ) .'}, {'word': \"['1']\", 'definition': '\"goodbye\"', 'sentence': 'this 1 better watch their back .'}, {'word': \"['10']\", 'definition': \"A person who's really hot, even more than others\", 'sentence': 'she was only the 10 in the crowd .'}, {'word': \"['101']\", 'definition': \"a beginner's course\", 'sentence': 'my boyfriend needs to re-take sex 101 .'}, {'word': \"['11']\", 'definition': 'an extremely attractive person - more attractive than a 10', 'sentence': \"dude , you have no chance with her . she 's like , an 11 .\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex = dsp_slang_train[20]\n",
        "demo = dsp_slang_train[20]\n",
        "demo.edited = \"clear for a phone call ?\"\n",
        "ex.demos = [demo]\n",
        "print(ex)\n",
        "print(slang_translate_template(ex))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDA2G61HVOi3",
        "outputId": "e60c688c-c9b3-4ebb-fa2f-41f36951817a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'word': 'charlie horse', 'definition': 'Leg Cramps or Muscle Cramps', 'sentence': 'in the middle of the night i stretched my leg and got a charlie horse in my calf.', 'demos': [{...}], 'edited': 'clear for a phone call ?'}\n",
            "Given a word and its definition, replace that word in the given sentence with the surrounding context.\n",
            "\n",
            "---\n",
            "\n",
            "Slang word: charlie horse\n",
            "Definition: Leg Cramps or Muscle Cramps\n",
            "Sentence: in the middle of the night i stretched my leg and got a charlie horse in my calf.\n",
            "Edited: clear for a phone call ?\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Slang word: ${slang word to be replaced}\n",
            "\n",
            "Definition: ${the definition of the word}\n",
            "\n",
            "Sentence: ${a sentence containing the word}\n",
            "\n",
            "Edited: ${same sentence but with the word replaced with its definition given the context}\n",
            "\n",
            "---\n",
            "\n",
            "Slang word: charlie horse\n",
            "\n",
            "Definition: Leg Cramps or Muscle Cramps\n",
            "\n",
            "Sentence: in the middle of the night i stretched my leg and got a charlie horse in my calf.\n",
            "\n",
            "Edited:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dsp.transformation\n",
        "\n",
        "def slang_dsp(example, k=2):\n",
        "  demo = dsp_slang_train[20]\n",
        "  demo.edited = \"in the middle of the night i stretched my leg and got a leg cramp in my calf\"\n",
        "  example.demos = [demo.copy()]\n",
        "  example, completions = dsp.generate(slang_translate_template)(example, stage='slang_translate')\n",
        "  return completions"
      ],
      "metadata": {
        "id": "ILem3TXX4cnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dsp.transformation\n",
        "\n",
        "def slang_dsp_final(example, k=2):\n",
        "  demo = dsp.Example(word=\"charlie horse\", definition=\"Leg Cramps or Muscle Cramps\", sentence=\"in the middle of the night i stretched my leg and got a charlie horse in my calf.\")\n",
        "  demo.edited = \"in the middle of the night i stretched my leg and got a leg cramp in my calf\"\n",
        "  example.demos = [demo.copy()]\n",
        "  example, completions = dsp.generate(slang_translate_template)(example, stage='slang_translate')\n",
        "  return completions"
      ],
      "metadata": {
        "id": "P8cI6ygj8llJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slang_dsp_final(dsp_slang_train[1], k=2).edited"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vfN6jqDMYdUg",
        "outputId": "5893fe36-a5d4-4fb4-b033-0802257259c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Aaaaahhhhh device used to block water ;; mothers against dyslexia ;; common misspelling of damn'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm.inspect_history(n=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BVnnPgpYtFV",
        "outputId": "9fafa561-8e87-419c-a136-437e1cd8c618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Given a word and its definition, replace that word in the given sentence with the surrounding context.\n",
            "\n",
            "---\n",
            "\n",
            "Slang word: charlie horse\n",
            "Definition: Leg Cramps or Muscle Cramps\n",
            "Sentence: in the middle of the night i stretched my leg and got a charlie horse in my calf.\n",
            "Edited: in the middle of the night i stretched my leg and got a leg cramp in my calf\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Slang word: ${slang word to be replaced}\n",
            "\n",
            "Definition: ${the definition of the word}\n",
            "\n",
            "Sentence: ${a sentence containing the word}\n",
            "\n",
            "Edited: ${same sentence but with the word replaced with its definition given the context}\n",
            "\n",
            "---\n",
            "\n",
            "Slang word: dam\n",
            "\n",
            "Definition: 1.device used to block water ;; 2.mothers against dyslexia ;; 3.common misspelling of damn\n",
            "\n",
            "Sentence: Aaaaahhhhh dam I am out of my favorite Tea Damit I have to wait until tomorrow to buy it now\n",
            "\n",
            "Edited:\u001b[32m Aaaaahhhhh device used to block water ;; mothers against dyslexia ;; common misspelling of damn\u001b[0m\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dsp_slang_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCxt_K9_eay0",
        "outputId": "c6a20dcf-5fe0-416e-edb4-1fdb4f70733d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dsp_slang_train))\n",
        "preds_df = pd.DataFrame()\n",
        "edited_sentences = []\n",
        "# words = []\n",
        "# sentences = []\n",
        "# definitions = []\n",
        "for ex in dsp_slang_train[:50]:\n",
        "  if ex.definition is not None and ex.word is not None:\n",
        "    print(ex.definition)\n",
        "    print(ex.word)\n",
        "    print(ex.sentence)\n",
        "    edited = slang_dsp_final(ex, k=2).edited\n",
        "  # words.append(ex.word)\n",
        "  # sentences.append(ex.sentence)\n",
        "  # definitions.append(ex.definition)\n",
        "    edited_sentences.append(edited)\n",
        "  else:\n",
        "    edited_sentences.append(ex.sentence)\n",
        "\n",
        "print(len(edited_sentences))\n",
        "preds_df['predictions'] = edited_sentences\n",
        "preds_df.to_csv('/content/drive/MyDrive/CS224U/sentiment_preds.csv', sep=',')"
      ],
      "metadata": {
        "id": "yS_gtPxpfwG6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "721ebfaf-81c1-4383-8678-f76537b988a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n",
            "a person's story: what they would have you believe\n",
            "drag\n",
            "Nicki s drag at the VMA s blessed my Monday\n",
            "1.device used to block water ;; 2.mothers against dyslexia ;; 3.common misspelling of damn\n",
            "dam\n",
            "Aaaaahhhhh dam  I am out of my favorite Tea  Damit I have to wait until tomorrow to buy it now   \n",
            "What Bostonians call their public transport system, almost always referring to the subway, but also including the various bus lines in the system.\n",
            "t\n",
            " I liked a  YouTube video http   t co BS9k9KKNP9 U2  Mick Jagger  Fergie     Gimmer Shelter   at the Rock and Roll Hall of Fame 25th \n",
            "the perineum\n",
            "twitter\n",
            "Spending tomorrow refreshing twitter because I want to go to the goddamn David Bowie musical   sorryboss\n",
            "The best and sparkliest character from the Rocky Horror Picture Show.\n",
            "columbia\n",
            "Whos going to shawn concert on October 16 in columbia sc bc we should totes meet\n",
            "sassy, beyond sassy\n",
            "posh\n",
            "New hair again tomorrow     hopefully look like this but unfortunately without the money  posh cars  david beckham    http   t co wXaL3RMrxj\n",
            "to leave a place to go somewhere else\n",
            "break\n",
            "The funniest part of the break comes when Briana exceed period 9 months being pregnant https   t co ZFfte4v6Mg\n",
            "(noun/verb) A description or the act of describing negative events in an overly favorable way\n",
            "spin\n",
            "British press spin the truth to claim Arsenal and Man City have made moves to sign Isco this January http   t co CVruBLkxzr\n",
            "The sensation you get upon realizing that the screenname you chose for Urban Dictionary is one that is used by many others and that all definitions created under that screenname are attributed to that single screenname.\n",
            "shame\n",
            "Some may find it tragic that I m sat alone watching Amy Schumer and eating Ben and Jerries but personally I feel no shame\n",
            "(syn) [Christina Aguilera]\n",
            "xtina\n",
            " m rutler  xtina  nbc ru hosting a Miss USA party   Sugar cake for the top 5   Miss America in AC in sept Free trip contests  Ur fired xtina\n",
            "Text messaging word for 'tickets'.\n",
            "tix\n",
            "I just got tix to Sam Smith on Oct 5  Let the countdown begin   LiveNation http   t co m2Omhs5D3Y\n",
            "Used by idiots who can't articulate correctly in place of the word 'have'. ;; Cross-reference: Often used in the same sentence as '[your]' when what they really mean to say is 'you're'. ;; In frequent use in England.\n",
            "of\n",
            "i m just gonna have to find a video audio of zayn s performance tomorrow\n",
            "a phrase indicating agreement or support\n",
            "hell yeah\n",
            "AC DC concert coming in September 8 at ford field already got me 2 tickets hell yeah  Just need to find me someone to go with   Center stage\n",
            "Somethign well-known and often likedby many ;; Prestigious ;; Popular\n",
            "famous\n",
            "I don t ever have any normal experiences with famous people   Check back on Friday as I take on Sam Smith s dressing rooms\n",
            "It means Grand Touring, usually the highest model of a car, can be found on many cars including the Mustang, and the Grand Am, also in italian can be translated into Gran Turismo, a famous racing game for PS2\n",
            "gt\n",
            "Hot Jam shouldn t have been on a Monday night  gt   lt  I have finals in the morning I can t go   \n",
            "An attempt to understand the world. ;; Often contains big words that are unnecessary.\n",
            "science\n",
            "weird as tho I have science 1 period Monday mornings and thanks to tom I have it instd amp still want Dunkin maybe the bus will stop tommorrow \n",
            "n. Short for 'Dominant.' The dominant person in a BDSM relationship or encounter.\n",
            "dom\n",
            "I sat down at IHOP ready to order  dom  amp  Joyce weren t ready and I said  ya know we all can t live life in the fast line \n",
            "when emphasized, indicates that the following noun is an exemplary example\n",
            "the\n",
            "So June 9th is the day  This is gonna be worse the the Galaxy Foams\n",
            "a dirty person who looks like crap\n",
            "scrap\n",
            "scrap my last tweet  At 5 22 on the 24th for Australian Directioners it will be two years for the boys\n",
            "when emphasized, indicates that the following noun is an exemplary example\n",
            "the\n",
            "I wanna go find the iPad so I can watch videos and train muh pokeymanz  rip iPod D   tomorrow marks two weeks\n",
            "Friend - See Also: son\n",
            "sun\n",
            "then it goes into here comes the sun from my star george harrison\n",
            "British Columbia- Most Western Canadian province. Also known for it's fine [word]BC bud[/word].\n",
            "bc\n",
            "I wish my old phone still worked bc it has videos on it from my 1st Shawn concert  amp all u can hear is me screaming  amp  crying in the background\n",
            "Machine that plays music! ;; Can be the label of lots of different things, such as jukebox on your computer.\n",
            "jukebox\n",
            "Guy at bar played That Hero Foo Fighters song on the jukebox and then sat at the bar and air drummed the whole thing\n",
            "\"homework\n",
            "hw\n",
            "my hw is due tomorrow  so here s some of the harry potter theme song http   t co iGKDHcM2i1\n",
            "(n) yet another Urban Dictionary entry prone to [Urban Censorship]\n",
            "disneyland\n",
            "I fucking went to disneyland today but they had a marathon going on and one entrance to the park  Sat in traffic for almost 3 hours\n",
            "a person with whom one is no longer in a relationship\n",
            "ex\n",
            " David Cameron shame on you and your government  As an ex Tory you disgust me  A soldier arrested over Bloody Sunday  Whilst IRA walk free \n",
            "a beginner's course\n",
            "101\n",
            "101  sure that I m going to have a break down tomorrow at Foo Fighters lol\n",
            "a dildo\n",
            "carrot\n",
            " Terrence Jones   Anthony Davis presented roses to the Derby winner  Meanwhile  Mike Marra   Peyton Siva gave the 3rd place horse a carrot  \n",
            "Term used by [DJ]s for when a record is playing, but not audible in the mix, while he is preparing and adjusting it to be played next.\n",
            "cue\n",
            "  cue rebecca black voice  it s friday  friday gotta get down on friday    \n",
            "\"cool\", \"neat\", \"awesome\"\n",
            "hard\n",
            " Goals for tomorrow  Big Mac  hard shell taco from Taco Bell  Dunkin coffee  AC Moore  gasoline  I think that s it  \n",
            "1.)Some one who you might think is attracting. ;; 2.)Physical attraction. ;; 3.)Cute, sexy, and what the person is wearin.\n",
            "cutie\n",
            " jsethriley Great what a cutie and it was nice to see your face on there omg Madonna coming back on stage 1st time I am on the internet\n",
            "not right\n",
            "off\n",
            "Kane may have bitten off more the he can chew\n",
            "great, excellent\n",
            "ace\n",
            " Finally  on the 5th attempt  Anderson wins the second set 6 3  serving his 12th ace for the match to lead 2 sets to love against Murray  \n",
            "a general exclamation not used to address anyone in particular, and can even be used when alone\n",
            "man\n",
            "Kendrick is gonna be  1 on every year end list   So vote for the man  yomilo and get the 1st round upset https   t co ybphzieskE\n",
            "great, excellent\n",
            "ace\n",
            "Also  ace time with the uni ladies  Saturday in Stonehaven  barbeque  Magic Mike XXL  tons of food    hips hurt from all the walking though \n",
            "\"thank you\n",
            "ta\n",
            "You just gotta ignite the light and let it shine just own the night like the 4th of July   Katy Perry\n",
            "to forget, ignore\n",
            "ig\n",
            "Tomorrow night everybody gone be that Chris brown picture on ig that says I don t want to go there because school the next day lol\n",
            "Something sensational, excellent or cool. ;; Part of 'what's cracking'.\n",
            "cracking\n",
            "Aj already seems to be cracking and all he did was go with my dad to pick them up from the airport   Briana and I may win this bet LOL\n",
            "a successful thing, especially a movie\n",
            "smash\n",
            "I literally just sat down to see Kris Bryant smash a bomb   LetsGo\n",
            "A state of violent mental agitation or wild excitement. ;; Temporary madness or delirium. ;; A mania; a craze.\n",
            "frenzy\n",
            "Disney whips Star Wars fans into frenzy with  Force Friday  event http   t co Fwz9DLOOK9\n",
            "Something that fails miserably\n",
            "flop\n",
            "Btw how is Kurt cobain a  flop  when he inspired millions and was in one of the most popular grunge bands   https   t co w45vqy8IFR\n",
            "1. (internet) The email folder where incoming mail arrives. ;; 2. (at the table) The mouth. ;; 3. (in bed) The vagina.\n",
            "inbox\n",
            "ok inbox on iOS has a pretty animation for the sun but thew android version doesnt   \n",
            "Hot High Quality Shit!!!\n",
            "gucci\n",
            "Debating if I should look hella gucci tomorrow or just gucci\n",
            "good exagerated\n",
            "great\n",
            "We are certainly gonna see Verratti in the 2nd half  I m like a kid waiting for his present     Oh  amp  Zlatan starts so thats great   PSGFIO\n",
            "The greatest and best song in the world.\n",
            "tribute\n",
            "Seeing a Fleetwood Mac tribute band tonight and a Selena tribute band tomorrow night  I am excited \n",
            "a very hard ball ;; able to do some damage\n",
            "baseball\n",
            " I may not watch as much baseball as I did when I was younger  but there s nothing like the Yankees beating the Red Sox   Classic \n",
            "The very best way to rent DVDs you order them online and they come in the mail no late fees, you can have 3 out at a time, and you don't have to deal with the asshole employees at Blockbuster. All for only $20 a month charged to your credit card.\n",
            "netflix\n",
            "I want to see EB but idk don t look to good guess ill call it a netflix night \n",
            "It means Grand Touring, usually the highest model of a car, can be found on many cars including the Mustang, and the Grand Am, also in italian can be translated into Gran Turismo, a famous racing game for PS2\n",
            "gt\n",
            " Signs and symptoms that may come with HIV infection  Once you see them  please  run for HIV TEST SEE HERE gt  gt http   t co 8MnYZXwOgC \n",
            "to have sex with\n",
            "make\n",
            "i m so curious about how they re gonna make Jurassic World into a 2nd movie\n",
            "word meaning 'hey!' or 'you there!' To get someone's attention, often in anger or when one is on crack.\n",
            "say\n",
            "gotta say  seannmileymoore did WOW me saturday with david bowie very theatrical he let the song speak for itself i am now a fan of his  D\n",
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dsp_slang_train))\n",
        "preds_df = pd.read_csv('/content/drive/MyDrive/CS224U/sentiment_preds.csv')\n",
        "# cur_preds_df = pd.DataFrame()\n",
        "edited_sentences = []\n",
        "# words = []\n",
        "# sentences = []\n",
        "# definitions = []\n",
        "for ex in dsp_slang_train[50:100]:\n",
        "  if ex.definition is not None and ex.word is not None:\n",
        "    if type(ex.definition) == int and not math.isnan(ex.definition):\n",
        "      print(ex.definition)\n",
        "      print(ex.word)\n",
        "      print(ex.sentence)\n",
        "      edited = slang_dsp_final(ex, k=2).edited\n",
        "      # words.append(ex.word)\n",
        "      # sentences.append(ex.sentence)\n",
        "      # definitions.append(ex.definition)\n",
        "      edited_sentences.append(edited)\n",
        "  else:\n",
        "    edited_sentences.append(ex.sentence)\n",
        "\n",
        "# cur_preds_df['predictions'] = edited_sentences\n",
        "preds_df = preds_df.append(pd.DataFrame(edited_sentences, columns=['predictions']), ignore_index=True)\n",
        "print(len(preds_df))\n",
        "preds_df.to_csv('/content/drive/MyDrive/CS224U/sentiment_preds.csv', sep=',')\n",
        "# print(len(edited_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlV28tQoe5cP",
        "outputId": "d14b49f1-248e-47d6-e977-275a99d1f365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n",
            "50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-3a68c384266e>:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  preds_df = preds_df.append(pd.DataFrame(edited_sentences, columns=['predictions']), ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_df = pd.read_csv('/content/drive/MyDrive/CS224U/sentiment_preds.csv')\n",
        "\n",
        "final_test_df = pd.read_csv('/content/drive/MyDrive/CS224U/final-test-df.csv')\n",
        "final_df_no_def = final_test_df[50:]['text']\n",
        "print(final_df_no_def.head(10))\n",
        "print(len(final_df_no_def))\n",
        "print(len(preds_df))\n",
        "\n",
        "preds_df = pd.concat([preds_df, final_df_no_def], ignore_index=True)\n",
        "preds_df.to_csv('/content/drive/MyDrive/CS224U/final-translated_sentiment.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YA5kmITB4Q8",
        "outputId": "bc1d579a-6c58-4367-9d2e-59a9130a7586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50     I have 2 say I thought the  emmys were great ...\n",
            "51     WNTonight I m disappointed that you didn t le...\n",
            "52    New York hit three homers during an eight run ...\n",
            "53    Check out this contest to win 1 of 5  130 Part...\n",
            "54      Laurynita Yeah girl  Disney tomorrow and Zac...\n",
            "55    Tomorrow we have yoga for early birds   Atmana...\n",
            "56     It s OK to celebrate   jul   even though you ...\n",
            "57     Watchman may have drowned on the job  On 16 0...\n",
            "58    i can talk about Malcolm X all day    like whe...\n",
            "59    It official u002c I will not be flying to Rale...\n",
            "Name: text, dtype: object\n",
            "450\n",
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "current_subset_df = pd.read_csv('/content/drive/MyDrive/CS224U/final-test-df.csv')\n",
        "sentiments = current_subset_df['sentiment']\n",
        "final_translated_df = pd.read_csv('/content/drive/MyDrive/CS224U/final-translated_sentiment.csv')\n",
        "final_translated_df['sentiment'] = sentiments\n",
        "final_translated_df.to_csv('/content/drive/MyDrive/CS224U/final-translated_sentiment.csv', sep=',')"
      ],
      "metadata": {
        "id": "qsXyqRpmFPjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm.inspect_history(n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9npcXmpRkA4",
        "outputId": "384f3d5a-94bd-405e-94bf-60ad4f0b634b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Given a word and its definition, replace that word in the given sentence with the surrounding context.\n",
            "\n",
            "---\n",
            "\n",
            "Slang word: 21\n",
            "Definition: a request for a phone call\n",
            "Sentence: clear for a 21 ?\n",
            "Edited: clear for a phone call?\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Slang word: ${slang word to be replaced}\n",
            "\n",
            "Definition: ${the definition of the word}\n",
            "\n",
            "Sentence: ${a sentence containing the word}\n",
            "\n",
            "Edited: ${same sentence but with the word replaced with its definition given the context}\n",
            "\n",
            "---\n",
            "\n",
            "Slang word: jungle karma\n",
            "\n",
            "Definition: It happens when an athlete or coach appears on [Jim Rome]'s radio show (nicknamed 'The Jungle') or his TV show and then wins their next game and most of the time has a great game individually to go along with it. ;; Jungle [karma] can also backfire if you back out on your commitment to come on. If you do that you lose the game, have a bad game, or get hurt for jerking with the karma.\n",
            "\n",
            "Sentence: athletes and coaches should never mess with the jungle karma.\n",
            "\n",
            "Edited:\u001b[32m athletes and coaches should never mess with the karma of the jungle.\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Given a word and its definition, replace that word in the given sentence with the surrounding context.\n",
            "\n",
            "---\n",
            "\n",
            "Slang word: 21\n",
            "Definition: a request for a phone call\n",
            "Sentence: clear for a 21 ?\n",
            "Edited: clear for a phone call?\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Slang word: ${slang word to be replaced}\n",
            "\n",
            "Definition: ${the definition of the word}\n",
            "\n",
            "Sentence: ${a sentence containing the word}\n",
            "\n",
            "Edited: ${same sentence but with the word replaced with its definition given the context}\n",
            "\n",
            "---\n",
            "\n",
            "Slang word: car-gasm\n",
            "\n",
            "Definition: The event in which a person (usually male) exhibits orgasm like symptoms in the presence of a rare and/or exquisetly modified automobile.\n",
            "\n",
            "Sentence: `` when nate saw that lambo sittin on 24 inch blades, he had a car-gasm.''\n",
            "\n",
            "Edited:\u001b[32m `` when nate saw that lambo sittin on 24 inch blades, he had an orgasm.''\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Given a word and its definition, replace that word in the given sentence with the surrounding context.\n",
            "\n",
            "---\n",
            "\n",
            "Slang word: 21\n",
            "Definition: a request for a phone call\n",
            "Sentence: clear for a 21 ?\n",
            "Edited: clear for a phone call?\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Slang word: ${slang word to be replaced}\n",
            "\n",
            "Definition: ${the definition of the word}\n",
            "\n",
            "Sentence: ${a sentence containing the word}\n",
            "\n",
            "Edited: ${same sentence but with the word replaced with its definition given the context}\n",
            "\n",
            "---\n",
            "\n",
            "Slang word: suicide blonde\n",
            "\n",
            "Definition: noun. ;; a bottle blonde who has bleach-fried his or her hair to a very light shade with peroxide because he or she is naturally dark-haired.\n",
            "\n",
            "Sentence: that chick is n't for real... she's a total suicide blonde\n",
            "\n",
            "Edited:\u001b[32m that chick is n't for real... she's a total bottle blonde\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Given a word and its definition, replace that word in the given sentence with the surrounding context.\n",
            "\n",
            "---\n",
            "\n",
            "Slang word: 21\n",
            "Definition: a request for a phone call\n",
            "Sentence: clear for a 21 ?\n",
            "Edited: clear for a phone call?\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Slang word: ${slang word to be replaced}\n",
            "\n",
            "Definition: ${the definition of the word}\n",
            "\n",
            "Sentence: ${a sentence containing the word}\n",
            "\n",
            "Edited: ${same sentence but with the word replaced with its definition given the context}\n",
            "\n",
            "---\n",
            "\n",
            "Slang word: courtesy\n",
            "\n",
            "Definition: 1. courteous behaviour ;; 2. a polite speech or action, especially one required by convention. ;; 3. a thoughtful action or gesture.\n",
            "\n",
            "Sentence: that asshole just came in my mouth, he was n't even polite enough to give a courtesy tap!\n",
            "\n",
            "Edited:\u001b[32m that asshole just came in my mouth, he was n't even polite enough to give a polite tap!\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Given a word and its definition, replace that word in the given sentence with the surrounding context.\n",
            "\n",
            "---\n",
            "\n",
            "Slang word: 21\n",
            "Definition: a request for a phone call\n",
            "Sentence: clear for a 21 ?\n",
            "Edited: clear for a phone call?\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Slang word: ${slang word to be replaced}\n",
            "\n",
            "Definition: ${the definition of the word}\n",
            "\n",
            "Sentence: ${a sentence containing the word}\n",
            "\n",
            "Edited: ${same sentence but with the word replaced with its definition given the context}\n",
            "\n",
            "---\n",
            "\n",
            "Slang word: marachan\n",
            "\n",
            "Definition: Female version of the [Wapanese].\n",
            "\n",
            "Sentence: the marachan had posters of sepiroth and spike ( from cowboy bebop ) all over her bedroom wall.\n",
            "\n",
            "Edited:\u001b[32m the female version of the [Wapanese] had posters of sepiroth and spike ( from cowboy bebop ) all over her bedroom wall.\u001b[0m\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset_df['predicted'] = edited_sentences"
      ],
      "metadata": {
        "id": "RreN3RA7DPRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gold standard"
      ],
      "metadata": {
        "id": "CuDjGUh-_ZO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm_gold = dsp.GPT3(model='text-davinci-003', api_key=openai_key)\n",
        "\n",
        "dsp.settings.configure(lm=lm_gold)\n",
        "\n",
        "# gold_df = pd.read_csv('/content/drive/MyDrive/CS224U/new_combined_data.csv')\n"
      ],
      "metadata": {
        "id": "4DiZdTjU_dE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# edited_sentences_gold = []\n",
        "# # shortened = dsp_slang_train[:5000]\n",
        "# # words = []\n",
        "# # sentences = []\n",
        "# # definitions = []\n",
        "# print(len(dsp_slang_train))\n",
        "# for ex in dsp_slang_train[:50]:\n",
        "#   edited = slang_dsp(ex, k=2).edited\n",
        "#   # words.append(ex.word)\n",
        "#   # sentences.append(ex.sentence)\n",
        "#   # definitions.append(ex.definition)\n",
        "#   edited_sentences_gold.append(edited)\n",
        "\n",
        "# print(len(edited_sentences_gold))\n",
        "\n",
        "\n",
        "# # gold_df['gold'] = edited_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "mxHGcwRmAaXL",
        "outputId": "4bbd97f9-27dd-4ef8-8882-7e386b60bfb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "932\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-56005a3e7b4e>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsp_slang_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdsp_slang_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0medited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslang_dsp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medited\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;31m# words.append(ex.word)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# sentences.append(ex.sentence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dsp/primitives/primitives.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdsp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdsp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-76c55ab9bbc0>\u001b[0m in \u001b[0;36mslang_dsp\u001b[0;34m(example, k)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"in the middle of the night i stretched my leg and got a leg cramp in my calf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdemos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompletions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdsp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslang_translate_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'slang_translate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompletions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dsp/primitives/predict.py\u001b[0m in \u001b[0;36mdo_generate\u001b[0;34m(example, stage, max_depth, original_example)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Generate and extract the fields.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mcompletions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mcompletions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompletions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompt, only_completed, return_sorted, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"logprobs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mchoices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/backoff/_sync.py\u001b[0m in \u001b[0;36mretry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mexception\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mmax_tries_exceeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtries\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_tries_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_choice_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\u001b[0m in \u001b[0;36mbasic_request\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;34m\"stringify_request\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             }\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_gpt3_turbo_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\u001b[0m in \u001b[0;36m_cached_gpt3_turbo_request_v2_wrapped\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mNotebookCacheMemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cached_gpt3_turbo_request_v2_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOpenAIObject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_cached_gpt3_turbo_request_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m_cached_call\u001b[0;34m(self, args, kwargs, shelving)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmust_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmap_mode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0;31m# Memmap the output at the first call to be consistent with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m         self.store_backend.dump_item(\n\u001b[1;32m    781\u001b[0m             [func_id, args_id], output, verbose=self._verbose)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\u001b[0m in \u001b[0;36m_cached_gpt3_turbo_request_v2\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"stringify_request\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stringify_request\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOpenAIObject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChatCompletion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     ) -> Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], bool, str]:\n\u001b[0;32m--> 288\u001b[0;31m         result = self.request_raw(\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m             \u001b[0m_thread_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_create_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             result = _thread_context.session.request(\n\u001b[0m\u001b[1;32m    597\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                 \u001b[0mabs_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    527\u001b[0m         }\n\u001b[1;32m    528\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    441\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    704\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1272\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dsp_slang_train))\n",
        "gold_preds_df = pd.DataFrame()\n",
        "gold_edited_sentences = []\n",
        "# words = []\n",
        "# sentences = []\n",
        "# definitions = []\n",
        "for ex in dsp_slang_train[:100]:\n",
        "  edited = slang_dsp(ex, k=2).edited\n",
        "  # words.append(ex.word)\n",
        "  # sentences.append(ex.sentence)\n",
        "  # definitions.append(ex.definition)\n",
        "  gold_edited_sentences.append(edited)\n",
        "\n",
        "print(len(gold_edited_sentences))\n",
        "gold_preds_df['gold'] = gold_edited_sentences\n",
        "gold_preds_df.to_csv('/content/drive/MyDrive/CS224U/gold.csv', sep=',')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3f5220-0235-4f67-ceb8-10bfb1412de0",
        "id": "GXJIY_nWw0xt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "932\n",
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dsp_slang_train))\n",
        "gold_preds_df = pd.read_csv('/content/drive/MyDrive/CS224U/gold.csv')\n",
        "# cur_preds_df = pd.DataFrame()\n",
        "gold_edited_sentences = []\n",
        "# words = []\n",
        "# sentences = []\n",
        "# definitions = []\n",
        "for ex in dsp_slang_train[450:500]:\n",
        "  edited = slang_dsp(ex, k=2).edited\n",
        "  # words.append(ex.word)\n",
        "  # sentences.append(ex.sentence)\n",
        "  # definitions.append(ex.definition)\n",
        "  gold_edited_sentences.append(edited)\n",
        "\n",
        "# cur_preds_df['predictions'] = edited_sentences\n",
        "gold_preds_df = gold_preds_df.append(pd.DataFrame(gold_edited_sentences, columns=['gold']), ignore_index=True)\n",
        "print(len(gold_preds_df))\n",
        "gold_preds_df.to_csv('/content/drive/MyDrive/CS224U/gold.csv', sep=',')\n",
        "# print(len(edited_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55396759-84f7-469f-a83c-c6a5d0f6fcdb",
        "id": "W7WXsl9sw0x4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "932\n",
            "500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-2772c3e48109>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gold_preds_df = gold_preds_df.append(pd.DataFrame(gold_edited_sentences, columns=['gold']), ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gold_preds_df = pd.read_csv('/content/drive/MyDrive/CS224U/gold.csv')\n",
        "sentences = [ex.sentence for ex in dsp_slang_train[:500]]\n",
        "print(sentences[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWDTh_3R3QGb",
        "outputId": "61ebf7bc-1fc3-4c60-ab85-b253e63313f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"awww man, i failed that midterm today... that's bunk! oh wait, i forgot about my other midterm. i have failed both midterms! that's not just bunk... that's badunkabunk!!!\", 'sheri moon is a hot lass', \"i was amazed to see jimmy huffing his dog's silent farts. he huffed as if posessed, and it made me sick to my stomach. i guess jimmy is a real fart huffer.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# edited_sentences_gold = []\n",
        "\n",
        "# print(len(dsp_slang_train))\n",
        "# for ex in dsp_slang_train[400:430]:\n",
        "#   edited = slang_dsp(ex, k=2).edited\n",
        "#   edited_sentences_gold.append(edited)\n",
        "\n",
        "# print(len(edited_sentences_gold))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnCxCH3YhvpE",
        "outputId": "1f3f076d-6c57-4911-9212-7bb5070cad72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.5s (openai.error.RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 99de56eac506a2a105b40b837ad59750 in your message.))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x7fcc18d49750> with kwargs {}\n",
            "400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(edited_sentences_gold[220:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ7W1z4oog4k",
        "outputId": "3517b5e3-25d8-4dba-988d-b4f619941f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"let's grab a six pack before we leave, there's nowhere to stop on the way and it's a boring trip without any beers imbibed while travelling in an automobile.\", \"the x-files is on... i hope the character who is somehow involved in the whole governmental conspiracy and always puffs on a cigarette every time he's on is in this one. hey look... there's the character on cnn... i think he's trying to take away more of my rights.\", \"Sentence: the dude: fuckin' quintana... that creep can roll, man. walter sobchak: yeah, but he's a pervert, dude. the dude: yeah. walter sobchak: no, he's a sex offender. with a record. he served 6 months in chino for exposing himself to an eight year old. the dude: oh! walter sobchak: when he moved to hollywood he had to go door to door to tell everyone he was a sick fuck who gets off on little children. donny: what's a... sick fuck who gets off on little children, walter? walter sobchak: shut the fuck up, donny.\", 'p: look at this amazing new homepage: www.aol.com i: no comment!', 'once again the crunkest section at Jackson State University, the Trumpet section, is cutting through.', \"`` well, i do n't know -- though flood remains one of The Pimpest...bestest...awsomest...sexxy....ACK! WE LOVES THEM!'s most popular albums, i prefer the sheer ebullient eccentricity of apollo 18.''\", 'p1- oh shit asher roth is mad cool p2- true that', \"I'm sorry, I cannot provide an edited version of this sentence as it contains inappropriate and offensive language.\", \"she's staring at his male genitalia.\", \"i'm so fucking poor i had to buy 3.5 grams of fucking swiggity for 20 to 50 dollars depending on quality.\", \"``have you heard the latest musical offering from Former-Beatle Paul McCartney?'' ``yeah its shit.''\", 'Sorry, I cannot provide an edited version of this sentence as it contains inappropriate and offensive language.', \"Slang word: Beyonce\\n\\nDefinition: A great singer. But on her cd, she keeps reminding us that she is a virgo. I don't care what your sign is, Beyonce.\\n\\nSentence: man: i love destiny's child especially Beyonce!! woman: what about kelly and michelle!!? man: who?\\n\\nEdited: man: i love destiny's child especially a great singer. But on her cd, she keeps reminding us that she is a virgo. I don't care what your sign is, Beyonce. woman: what about kelly and michelle!!? man: who?\", 'get out of bed, you lazy person!', 'The relationship between sound and time is life.', 'non-irc user: fuck you irc \"lamers\" are fucking lame. irc user: an abrupt and lame way to end a conversation.', \"how Southern for the New Yorker's 'youse guyse' doin?\", 'anyone got some hentai featuring the Teen Titans character Raven?', \"`` do you want to cuddle, or do you want the learning disability or one of your dumbass friends?''\", 'we giggled uncontrollably as Kate forcibly stripped Ryan of his trousers in the middle of the crowded mall!', \"interesting trivia for ya: the band the jet powered strategic bombers still in sevice after 50 years, and still kicking some ass, dont fuck with us ;; and if you dont like it, that's tough beans go crys are not named after the hairstyle, but rather the bomber... which looks like the jet powered strategic bombers nosecone pham tuan, the first pilot to shoot down a stratofortress, later became vietnam's first cosmonaut. the jet powered strategic bombers can carry around 70,000lbs of bombs. thats right, seventy-fucking-thousand. the jet powered strategic bombers is so old, that its perfectly possible for a pilot out there to be flying the same buf that his father and grandfather previously\", 'At the Havoc concert front row was a member of the movement/organization of empowered womyn (and occasionally men) dedicated to expressing radical, grassroots feminism through art and activism, she also has her own band, and has never let a guy tell her what to do.', \"that milf's [bored out] probably hangs halfway down her thighs.\", \"time to go to the launderette: my special socks that are used for 'mopping up' following a 'five knuckle shuffle' are n't mopping up anymore.\", \"do n't flatulate on my bed!\", \"Slang word: yakward\\n\\nDefinition: 3) A word to describe a ten-second void of any speech or thought during a friendly group conversation due to an individual's awkward comment.\\n\\nSentence: -hey guys. wan na hann gout? - * insert awkward comment in any given fun conversation * now wait 10 seconds. follow up with a change in topic in hope of losing or disintersting submitter of awkward comment -person 1: `` shit man, my parents want to talk to me about sex and sleeping naked with girls.'' person 2: man that's fuckin' a ten-second void of any speech or thought during a friendly group conversation due to an individual's awkward comment.\", '``I knew you wouldn\\'t get it right...\\'\\' reply: \"Say \\'shutup\\' in a non-offensive or amicable manner, ya sap!\"', 'I went to Minnesota Mining and Manufacturing to buy some sandpaper and mistakenly called Duck tape.', 'you son of a homosexual man', 'That party was of poor quality, disappointing, the opposite of \"massive\".']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset_df['gold'] = edited_sentences_gold"
      ],
      "metadata": {
        "id": "KIDZGkNr1C6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_df.to_csv(\"/content/drive/MyDrive/CS224U/cs224u-translated_slang.csv\", sep=\",\")"
      ],
      "metadata": {
        "id": "FIlbhsSXAlHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate the EM and F1"
      ],
      "metadata": {
        "id": "vfgyYRCjX4nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EM\n",
        "gold_df = pd.read_csv('/content/drive/MyDrive/CS224U/gold.csv')\n",
        "pred_df = pd.read_csv('/content/drive/MyDrive/CS224U/preds.csv')\n",
        "gold_sentences = gold_df['gold']\n",
        "pred_sentences = pred_df['predictions']\n",
        "\n"
      ],
      "metadata": {
        "id": "IDwQ6oN4X4UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def calculate_em_score(predicted_sentence, gold_sentence):\n",
        "    # Preprocess and tokenize the sentences\n",
        "    predicted_tokens = re.findall(r'\\w+|[^\\w\\s]', predicted_sentence.lower())\n",
        "    gold_tokens = re.findall(r'\\w+|[^\\w\\s]', gold_sentence.lower())\n",
        "\n",
        "    # print(predicted_tokens)\n",
        "    # print(gold_tokens)\n",
        "\n",
        "    # Compare the tokens\n",
        "    if predicted_tokens == gold_tokens:\n",
        "        return 1.0\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "def calculate_average_em_score(predicted_sentences, gold_sentences):\n",
        "    total_em_score = 0.0\n",
        "    num_examples = len(predicted_sentences)\n",
        "\n",
        "    for predicted_sentence, gold_sentence in zip(predicted_sentences, gold_sentences):\n",
        "        em_score = calculate_em_score(predicted_sentence, gold_sentence)\n",
        "        total_em_score += em_score\n",
        "\n",
        "    average_em_score = total_em_score / num_examples\n",
        "    return average_em_score"
      ],
      "metadata": {
        "id": "j0JUZjSbKQB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_em_score = calculate_average_em_score(pred_sentences, gold_sentences)\n",
        "print(\"Average EM score:\", average_em_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f35_KnjKSSF",
        "outputId": "7c24e0a3-efaf-42d5-8192-6c150d0ca274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average EM score: 0.222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# F1\n",
        "\n",
        "def calculate_f1_score(predicted_sentence, gold_sentence):\n",
        "    # Preprocess and tokenize the sentences\n",
        "    predicted_tokens = re.findall(r'\\w+|[^\\w\\s]', predicted_sentence.lower())\n",
        "    gold_tokens = re.findall(r'\\w+|[^\\w\\s]', gold_sentence.lower())\n",
        "\n",
        "    # Calculate true positives, false positives, and false negatives\n",
        "    true_positives = len(set(predicted_tokens) & set(gold_tokens))\n",
        "    false_positives = len(predicted_tokens) - true_positives\n",
        "    false_negatives = len(gold_tokens) - true_positives\n",
        "\n",
        "    # Calculate precision, recall, and F1 score\n",
        "    precision = true_positives / (true_positives + false_positives)\n",
        "    recall = true_positives / (true_positives + false_negatives)\n",
        "    # print(precision)\n",
        "    # print(recall)\n",
        "\n",
        "\n",
        "    if precision + recall == 0:\n",
        "        f1_score = 0.0\n",
        "    else:\n",
        "        f1_score = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    return f1_score, precision, recall\n",
        "\n",
        "def calculate_average_f1_score(predicted_sentences, gold_sentences):\n",
        "    total_f1_score = 0.0\n",
        "    total_precision = 0.0\n",
        "    total_recall = 0.0\n",
        "    num_examples = len(predicted_sentences)\n",
        "\n",
        "    for predicted_sentence, gold_sentence in zip(predicted_sentences, gold_sentences):\n",
        "        f1_score, precision, recall = calculate_f1_score(predicted_sentence, gold_sentence)\n",
        "        total_f1_score += f1_score\n",
        "        total_precision += precision\n",
        "        total_recall += recall\n",
        "\n",
        "    average_f1_score = total_f1_score / num_examples\n",
        "    average_precision = total_precision / num_examples\n",
        "    average_recall = total_recall / num_examples\n",
        "    return average_f1_score, average_precision, average_recall"
      ],
      "metadata": {
        "id": "yG92AW1fK3ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_f1_score, average_precision, average_recall = calculate_average_f1_score(pred_sentences, gold_sentences)\n",
        "print(\"Average F1 score:\", average_f1_score)\n",
        "print(\"Average Precision score:\", average_precision)\n",
        "print(\"Average Recall score:\", average_recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1G1DJ_wMGGQ",
        "outputId": "f4de1ad9-2789-445b-e1ec-6f2c63eca426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average F1 score: 0.6842589945241122\n",
            "Average Precision score: 0.7474535656770882\n",
            "Average Recall score: 0.6673137035000308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O-pze-FAML4B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}